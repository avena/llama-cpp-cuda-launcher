# LLaMA.cpp CUDA Tools ‚Äì Modelos de Linguagem Local com GPU NVIDIA

Scripts PowerShell para Windows que automatizam instala√ß√£o do **llama.cpp** com CUDA, download de modelos GGUF e inicializa√ß√£o de servidor compat√≠vel com a API OpenAI para uso com **Cline** e **Continue.dev** no VS Code.

---

## Tabela de Conte√∫dos

- [Configura√ß√£o do Ambiente](#configura√ß√£o-do-ambiente)
- [Quick Start](#quick-start)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Instala√ß√£o](#instala√ß√£o)
- [Modelos](#modelos)
- [Execu√ß√£o do Servidor](#execu√ß√£o-do-servidor)
- [Integra√ß√£o com VS Code](#integra√ß√£o-com-vs-code)
- [Conceitos](#conceitos)
- [Ferramentas Avan√ßadas](#ferramentas-avan√ßadas)
- [TODO ‚Äì Modelos a Explorar](#todo--modelos-a-explorar)
- [Solu√ß√£o de Problemas](#solu√ß√£o-de-problemas)
- [Changelog](#changelog)

---

## Configura√ß√£o do Ambiente

Configure seu ambiente Windows antes de come√ßar. Esta se√ß√£o instala o PowerShell 7, Scoop e aria2c.

### Passo 1 ‚Äî PowerShell 7

```powershell
# Instalar PS7
winget install Microsoft.PowerShell

# Definir como padr√£o
powershell -ExecutionPolicy Bypass -File .\Set-PowerShell7AsDefault.ps1
```

### Passo 2 ‚Äî Scoop (opcional mas recomendado)

Scoop √© um gerenciador de pacotes para Windows que facilita a instala√ß√£o e atualiza√ß√£o de ferramentas como aria2c.

```powershell
# Instalar Scoop (execute em PowerShell 7)
irm get.scoop.sh | iex

# Adicionar bucket principal
scoop bucket add main
```

### Passo 3 ‚Äî aria2c

Escolha uma das op√ß√µes abaixo para instalar o aria2c (usado para downloads r√°pidos):

| M√©todo     | Comando                      | Observa√ß√£o                             |
| ---------- | ---------------------------- | -------------------------------------- |
| **winget** | `winget install aria2.aria2` | M√©todo oficial Microsoft               |
| **scoop**  | `scoop install aria2`        | Recomendado se Scoop estiver instalado |

> **Por que usar aria2c?** Downloads paralelos e resum√≠veis, essencial para baixar modelos GGUF de v√°rios GB rapidamente.

---

## Quick Start

Para quem j√° tem o ambiente configurado ‚Äî os 4 comandos do dia a dia:

```powershell
# 1. Baixar modelo (primeira vez)
.\setup-models.ps1

# 2. Iniciar servidor
.\start-llama-server.ps1

# 3. Testar (opcional, em outro terminal)
cp .env.example .env
.\run-test.ps1

# 4. Usar no VS Code
# Cline ‚Üí OpenAI Compatible ‚Üí http://127.0.0.1:8080/v1
```

---

## Pr√©-requisitos

- Windows 10/11 com PowerShell 5.1+ (recomendado PowerShell 7)
- GPU NVIDIA com suporte a CUDA 12.4 ou 13.1
- M√≠nimo **6 GB de VRAM**
- ~5 GB de espa√ßo em disco

> **Nota:** A configura√ß√£o do PowerShell 7, Scoop e aria2c est√° descrita na se√ß√£o [Configura√ß√£o do Ambiente](#configura√ß√£o-do-ambiente).

---

## Instala√ß√£o

### Passo 1 ‚Äî Instalar o llama.cpp

O script abaixo detecta automaticamente a vers√£o de CUDA suportada pelo seu driver e usa aria2c para download paralelo:

```powershell
powershell -ExecutionPolicy Bypass -File .\setup_llama_cpp_cuda12_cuda_13_aria2c.ps1
```

O script ir√°:

- Detectar CUDA via `nvidia-smi` (suporta 12.4 e 13.1)
- Baixar bin√°rios pr√©-compilados + DLLs do CUDA Runtime
- Extrair para `[DISCO]:\llama-cpp-cuda124` ou `cuda131`

<details>
<summary>Alternativas de instala√ß√£o (scripts legados)</summary>

| Script                               | Quando usar                     |
| ------------------------------------ | ------------------------------- |
| `setup_llama_cpp_cuda12_cuda_13.ps1` | Sem aria2c, detec√ß√£o autom√°tica |
| `setup_llama_cpp_cuda124.ps1`        | CUDA 12.4 fixo, sem aria2c      |
| `setup_llama_cpp_cuda124_aria2c.ps1` | CUDA 12.4 fixo, com aria2c      |

</details>

### Passo 2 ‚Äî PowerShell 7, Scoop e aria2c

Consulte a se√ß√£o [Configura√ß√£o do Ambiente](#configura√ß√£o-do-ambiente) para instalar PowerShell 7, Scoop e aria2c.

---

## Modelos

### Como baixar

```powershell
.\setup-models.ps1
# Menu interativo ‚Äî escolha o modelo desejado
```

Scripts diretos para automa√ß√£o:

```powershell
.\setup-qwen2.5-coder-0.5b.ps1            # ~379 MB
.\setup-deepseek-coder-6.7b-instruct.ps1  # ~4.08 GB
```

> O **Qwen2.5-3B-Instruct** n√£o tem script dedicado ainda. Baixe manualmente em
> `bartowski/Qwen2.5-3B-Instruct-GGUF` no Hugging Face e salve em
> `[DISCO]:\models-ai\qwen2.5-3b-instruct\`.

<details>
<summary>Estrutura de pastas esperada</summary>

```
C:\models-ai\
‚îú‚îÄ‚îÄ qwen2.5-coder-0.5b-instruct\
‚îÇ   ‚îî‚îÄ‚îÄ qwen2.5-coder-0.5b-instruct-q4_k_m.gguf
‚îú‚îÄ‚îÄ qwen2.5-3b-instruct\
‚îÇ   ‚îî‚îÄ‚îÄ qwen2.5-3b-instruct-q4_k_m.gguf
‚îî‚îÄ‚îÄ deepseek-coder-6.7b-instruct\
    ‚îî‚îÄ‚îÄ deepseek-coder-6.7b-instruct-q4_k_m.gguf
```

</details>

### Qual modelo escolher

| Modelo                            | Tamanho | VRAM    | Tokens/s (RTX 4050) | Ideal para                       |
| --------------------------------- | ------- | ------- | ------------------- | -------------------------------- |
| Qwen2.5-Coder-0.5B Q4_K_M         | 379 MB  | ~0.8 GB | >80 t/s             | Testes r√°pidos, prot√≥tipos       |
| ‚úÖ **Qwen2.5-3B-Instruct Q4_K_M** | ~1.9 GB | ~2 GB   | **~58 t/s**         | **Uso geral ‚Äî recomendado**      |
| Qwen2.5-Coder-7B Q4_K_M           | ~4.7 GB | ~5.3 GB | 15‚Äì25 t/s           | C√≥digo, melhor qualidade         |
| DeepSeek-Coder-6.7B Q4_K_M        | ~4.0 GB | ~5.8 GB | ~6 t/s              | Produ√ß√£o (hardware mais potente) |

> **Por que o Qwen2.5-3B √© o sweet spot na RTX 4050?**
> Com ~2 GB de VRAM, deixa ~4 GB livres para cache KV ‚Äî roda 100% na GPU com contexto longo sem spill para RAM.
> O DeepSeek-6.7B ocupa ~5.8 GB dos 6 GB dispon√≠veis, resultando em ~9,5x menos tokens/s no benchmark real.

### Benchmark Real ‚Äî RTX 4050 Notebook (6 GB VRAM)

> Teste: `"Write a Python function that receives a list of integers and returns only the even numbers"`

| Modelo                         | Tokens | Tempo | Tokens/s       |
| ------------------------------ | ------ | ----- | -------------- |
| deepseek-coder-6.7b Q4_K_M     | 86     | 14s   | 6,11 t/s       |
| **Qwen2.5-3B-Instruct Q4_K_M** | 263    | 4,5s  | **~58 t/s** ‚úÖ |

> RTX 4050 notebook tem **192 GB/s** de bandwidth de mem√≥ria. Infer√™ncia em LLMs √© muito sens√≠vel a isso.

### Quantiza√ß√£o ‚Äî IQ vs K vs UD

**K-quants (Q4_K_M, Q5_K_M)** ‚Äî padr√£o confi√°vel, ampla compatibilidade. `_M` aplica bits extras seletivamente nas camadas mais sens√≠veis. Escolha segura para come√ßar.

**IQ-quants (IQ4_XS, IQ3_M)** ‚Äî tabelas de lookup n√£o-lineares, arquivo ~10% menor que Q4_K_M com qualidade igual ou superior. Para modelos ‚â§7B a diferen√ßa √© mais percept√≠vel.

**UD ‚Äî Unsloth Dynamic** ‚Äî precis√£o mista por camada (cr√≠ticas em 8-bit, demais em 2‚Äì3 bits). Qualidade pr√≥xima do Q8, tamanho menor que Q4. Dispon√≠vel em menos modelos.

| Quantiza√ß√£o | Qualidade  | Tamanho | Recomenda√ß√£o            |
| ----------- | ---------- | ------- | ----------------------- |
| Q5_K_M      | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Maior   | Se sobrar VRAM          |
| Q4_K_M      | ‚≠ê‚≠ê‚≠ê‚≠ê   | M√©dio   | ‚úÖ Padr√£o seguro        |
| IQ4_XS      | ‚≠ê‚≠ê‚≠ê‚≠ê+  | Menor   | Melhor para modelos ‚â§7B |
| UD-Q4       | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Menor   | Melhor custo/benef√≠cio  |
| Q3_K_M      | ‚≠ê‚≠ê‚≠ê     | Pequeno | Se VRAM for cr√≠tica     |

---

## Execu√ß√£o do Servidor

### Op√ß√£o A ‚Äî Uso Geral ‚≠ê Recomendado

```powershell
powershell -ExecutionPolicy Bypass -File .\start-llama-server.ps1
```

Menu interativo com 4 modelos, par√¢metros configur√°veis e suporte a LAN simult√¢neo.

| Par√¢metro       | Qwen2.5-0.5B | Qwen2.5-3B | Qwen2.5-7B | DeepSeek-6.7B |
| --------------- | ------------ | ---------- | ---------- | ------------- |
| Context         | 16384        | 32768      | 32768      | 16384         |
| VRAM Est.       | ~0.8 GB      | ~2 GB      | ~5.3 GB    | ~5.8 GB       |
| GPU Layers      | 999          | 999        | 999        | 35            |
| Max Tokens      | 512          | 2048       | 2048       | 2048          |
| Flash Attention | ‚úÖ           | ‚úÖ         | ‚úÖ         | ‚úÖ            |

Output ao iniciar:

```
Endpoints - este PC:   http://127.0.0.1:8080
Endpoints - LAN:       http://192.168.1.100:8080
Config Cline/Continue: Base URL: http://127.0.0.1:8080/v1
                       API Key:  sk-no-key-required
```

### Op√ß√£o B ‚Äî RTX 4050 Otimizado

```powershell
powershell -ExecutionPolicy Bypass -File .\start-llama-server-rtx4050.ps1
```

Espec√≠fico para Ada Lovelace (RTX 40xx) com 6 GB. Ativa:

- `--flash-attn` ‚Äî reduz VRAM do cache KV em ~40%
- `--cache-type-k/v q8_0` ‚Äî KV cache em 8-bit (~50% menos VRAM vs FP16)
- `--n-gpu-layers 99` ‚Äî offload total para GPU
- `--batch-size 2048` ‚Äî otimizado para Tensor Cores

> Context do DeepSeek reduzido para 8192 para caber nos 6 GB.

<details>
<summary>Op√ß√£o C ‚Äî Script com todos os 4 modelos</summary>

Script completo com todos os modelos dispon√≠veis (Qwen2.5-0.5B, Qwen2.5-3B, Qwen2.5-Coder-7B, DeepSeek-6.7B). Recomendado para quem quer acesso a todos os modelos.

```powershell
powershell -ExecutionPolicy Bypass -File .\start-llama-server.ps1
```

</details>

---

## Integra√ß√£o com VS Code

### Cline

1. Abra as configura√ß√µes do Cline ‚Üí **Connect to a model** ‚Üí **OpenAI Compatible**
2. Preencha os campos:

| Campo      | Valor                             |
| ---------- | --------------------------------- |
| Base URL   | `http://127.0.0.1:8080/v1`        |
| API Key    | `sk-no-key-required`              |
| Model      | `qwen2.5-3b-instruct-q4_k_m.gguf` |
| Max Tokens | `2048`                            |

### Continue.dev

O reposit√≥rio j√° inclui `.continue/continue.config.yaml` pr√©-configurado ‚Äî nenhuma edi√ß√£o necess√°ria. Instale a extens√£o e abra o workspace.

```yaml
models:
  - name: DeepSeek Coder Local
    provider: openai
    model: deepseek-coder-6.7b-instruct-q4_k_m.gguf
    apiBase: http://127.0.0.1:8080/v1
    apiKey: sk-no-key-required
```

### Acesso via LAN

O script Op√ß√£o A escuta em `0.0.0.0`, permitindo acesso de outros dispositivos na mesma rede:

```powershell
# Descobrir seu IP local
ipconfig | findstr "IPv4"
# Acessar de outro PC: http://[SEU-IP]:8080/v1
```

---

## Conceitos

### Context Window vs Max Tokens

```
Context do servidor (ex: 16384 tokens)
‚îú‚îÄ‚îÄ Hist√≥rico da conversa:   ~8288 tokens
‚îú‚îÄ‚îÄ Sua mensagem atual:      ~2000 tokens
‚îî‚îÄ‚îÄ Max Tokens (resposta):   4096 tokens  ‚Üê configure no Cline
                             ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                             TOTAL: 14384 ‚úî
```

- **Context alto** ‚Üí mais mem√≥ria de conversa, mais VRAM, mais lento
- **Reduza para `8192`** se a resposta estiver lenta ou se houver swap para RAM
- **Max Tokens no Cline**: recomendado `2048` (Qwen2.5) ou `4096` (DeepSeek)

---

## Ferramentas Avan√ßadas

### Framework de Testes

```powershell
# Terminal 1 ‚Äî inicia o servidor
.\start-llama-server_qwen2.5-3b.ps1

# Terminal 2 ‚Äî executa o teste
cp .env.example .env
.\run-test.ps1
# Detecta o modelo ativo automaticamente e executa o script correspondente
```

**M√©tricas geradas:**

| M√©trica         | Descri√ß√£o                                       |
| --------------- | ----------------------------------------------- |
| Prompt Speed    | Velocidade de leitura do prompt (tokens/s)      |
| Predict Speed   | Velocidade de gera√ß√£o (tokens/s)                |
| Total Inference | Tempo total (ms)                                |
| Stop Reason     | `eos` = terminou natural ‚úî / `limit` = truncado |

**Scripts de teste dispon√≠veis:**

| Script                               | Modelo                    |
| ------------------------------------ | ------------------------- |
| `tests/test-qwen2.5-3b.ps1`          | Qwen2.5-3B-Instruct       |
| `tests/test-qwen2.5-coder-0.5b.ps1`  | Qwen2.5-Coder-0.5B        |
| `tests/test-qwen2.5-coder-7b.ps1`    | Qwen2.5-Coder-7B-Instruct |
| `tests/test-deepseek-coder-6.7b.ps1` | DeepSeek-Coder-6.7B       |

### Vari√°veis de Ambiente (.env)

```powershell
cp .env.example .env
```

| Vari√°vel              | Padr√£o                  | Descri√ß√£o                 |
| --------------------- | ----------------------- | ------------------------- |
| `LLAMA_SERVER_URL`    | `http://localhost:8080` | URL do servidor           |
| `DEFAULT_MAX_TOKENS`  | `2048`                  | Tokens m√°ximos na gera√ß√£o |
| `DEFAULT_TEMPERATURE` | `0.2`                   | Temperatura padr√£o        |

---

## TODO ‚Äì Modelos a Explorar

> Modelos que podem rodar na RTX 4050 (6 GB VRAM) com quantiza√ß√µes adequadas ou que requerem hardware futuro.

### ‚úÖ Em uso / Testados

- [x] **`deepseek-coder-6.7b` Q4_K_M** ‚Äì ~6 t/s na RTX 4050 (funcional, lento)
- [x] **`Qwen2.5-3B-Instruct` Q4_K_M** ‚Äì ~58 t/s ‚úÖ recomendado atualmente

### üîú Pr√≥ximos a testar (cabem na RTX 4050 ‚Äì 6 GB)

#### DeepSeek ‚Äì Modelos Distilled

- [ ] **`DeepSeek-R1-Distill-Qwen-1.5B` Q4_K_M** (~1.1 GB, ~1.3 GB VRAM)
  - Destilado do R1 671B com racioc√≠nio chain-of-thought
  - Estimado: 25‚Äì43 t/s | HF: `bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF`

- [ ] **`DeepSeek-R1-Distill-Qwen-7B` Q4_K_M** (~4.7 GB, ~5.5 GB VRAM)
  - Estimado: 10‚Äì20 t/s | ‚ö†Ô∏è Testar com context 4096‚Äì8192
  - HF: `bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF`

- [ ] **`DeepSeek-R1-Distill-Llama-8B` Q4_K_M** (~5.0 GB, ~5.8 GB VRAM)
  - Estimado: 8‚Äì18 t/s | ‚ö†Ô∏è Mesma limita√ß√£o de VRAM do Qwen-7B
  - HF: `bartowski/DeepSeek-R1-Distill-Llama-8B-GGUF`

- [ ] **`DeepSeek-R1-0528-Qwen3-1.5B` Q4_K_M** (~1.1 GB)
  - AIME 2025: 70% ‚Üí 87,5% vs vers√£o anterior
  - HF: `unsloth/DeepSeek-R1-0528-Qwen3-1.5B-GGUF`

#### Qwen2.5 ‚Äì Pendentes

- [ ] **`qwen2.5-coder-3b` Q4_K_M** (~1.9 GB) ‚Äî comparar vs Qwen2.5-3B-Instruct no benchmark de c√≥digo
- [ ] **`qwen2.5-coder-7b` Q4_K_M** (~4.7 GB) ‚Äî ‚ö†Ô∏è pouco espa√ßo para cache KV nos 6 GB
- [ ] **`Qwen2.5-3B-Instruct` IQ4_XS** (~1.7 GB) ‚Äî ~10% menor, levemente mais r√°pido
  - HF: `bartowski/Qwen2.5-3B-Instruct-GGUF` ‚Üí arquivo `*IQ4_XS*`

### üî≠ Futuros / Requer hardware al√©m dos 6 GB

#### DeepSeek

- [ ] **`DeepSeek-R1-Distill-Qwen-14B` Q4_K_M** (~9 GB VRAM) ‚Äî requer RTX 4060 Ti 16GB+
- [ ] **`DeepSeek-V3.1` Q2/IQ2** (MoE 671B total, ~37B ativos)
  - Thinking + non-thinking em um modelo, tool calling melhorado para agentes
  - HF: `unsloth/DeepSeek-V3.1-GGUF`
- [ ] **`DeepSeek-R1-0528`** (vers√£o completa 671B) ‚Äî hardware enterprise

#### Qwen3

- [ ] **`Qwen3-Coder-Next` UD-IQ3_XXS** (MoE ‚Äì 80B total, **3B ativos**) ‚≠ê
  - Velocidade de modelo pequeno, qualidade de modelo grande | Context: 256K tokens
  - Estimado: 20‚Äì40 t/s com 24+ GB VRAM | ‚ö†Ô∏è N√£o roda na RTX 4050
  - HF: `unsloth/Qwen3-Coder-Next-GGUF`

- [ ] **`Qwen3-Coder-480B-A35B` UD-Q2** (MoE ‚Äì 480B total, 35B ativos)
  - 7.5T tokens de treino (70% c√≥digo) | Context: 256K‚Äì1M tokens | hardware enterprise
  - HF: `unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF`

### üõ†Ô∏è Tarefas de Scripts

- [ ] `setup-models.ps1` com op√ß√£o para R1-distilled (1.5B e 7B)
- [ ] `tests/test-deepseek-r1-distill-1.5b.ps1` com prompts de racioc√≠nio
- [ ] `tests/test-deepseek-r1-distill-7b.ps1`
- [x] `tests/test-qwen2.5-coder-7b.ps1` ‚úÖ
- [ ] `tests/test-qwen2.5-coder-3b.ps1`
- [ ] `start-llama-server_qwen2.5-3b.ps1` ‚Äî op√ß√£o `[4]` qwen2.5-coder-7b e `[5]` deepseek-r1-distill-1.5b
- [ ] `run-benchmark-all.ps1` ‚Äî testa todos os modelos instalados e gera tabela comparativa de t/s

---

## Solu√ß√£o de Problemas

| Problema                | Solu√ß√£o                                                                          |
| ----------------------- | -------------------------------------------------------------------------------- |
| `aria2c n√£o encontrado` | `winget install aria2.aria2` e reiniciar terminal                                |
| GPU n√£o reconhecida     | Verificar `nvidia-smi` e `nvcc --version`                                        |
| Porta 8080 ocupada      | `netstat -an \| findstr 8080`                                                    |
| DeepSeek muito lento    | RTX 4050 tem bandwidth limitado; trocar para Qwen2.5-3B                          |
| Resposta truncada       | Stop reason `limit` ‚Äî aumentar `DEFAULT_MAX_TOKENS` no `.env`                    |
| Cline n√£o conecta       | `Invoke-WebRequest http://127.0.0.1:8080/health` deve retornar `{"status":"ok"}` |

```powershell
# Logs em tempo real
Get-Content $env:TEMP\llama-server.log -Tail 50 -Wait

# Monitorar VRAM
nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.free --format=csv -l 2
```

---

## Refer√™ncias

- [llama.cpp GitHub](https://github.com/ggml-org/llama.cpp)
- [bartowski ‚Äì GGUF no Hugging Face](https://huggingface.co/bartowski)
- [unsloth ‚Äì GGUF no Hugging Face](https://huggingface.co/unsloth)
- [Qwen2.5-3B-Instruct-GGUF](https://huggingface.co/bartowski/Qwen2.5-3B-Instruct-GGUF)
- [aria2c](https://aria2.github.io/)
- [Cline ‚Äì VS Code](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)
- [Continue.dev ‚Äì VS Code](https://marketplace.visualstudio.com/items?itemName=Continue.continue)
