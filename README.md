# LLaMA.cpp CUDA Tools â€“ Execute Modelos de Linguagem Localmente com GPU NVIDIA

Conjunto completo de scripts PowerShell para Windows que automatiza a instalaÃ§Ã£o do **llama.cpp** com aceleraÃ§Ã£o CUDA, download de modelos GGUF do Hugging Face e inicializaÃ§Ã£o de um servidor compatÃ­vel com a API OpenAI para integraÃ§Ã£o com extensÃµes como **Cline** e **Continue.dev** no VS Code.

## VisÃ£o Geral

Este projeto Ã© construÃ­do sobre **quatro pilares fundamentais**:

1. **llama.cpp + CUDA 12.4** - InferÃªncia de modelos locais com aceleraÃ§Ã£o de GPU NVIDIA
2. **Modelos GGUF** - Modelos otimizados e quantizados (DeepSeek Coder, Qwen2.5-Coder)
3. **API OpenAI compatÃ­vel** - Servidor que expÃµe a mesma interface que a OpenAI
4. **IntegraÃ§Ã£o com IDEs** - Uso direto via Cline ou Continue.dev no VS Code

### OpÃ§Ãµes de IntegraÃ§Ã£o

- **Cline** - ExtensÃ£o popular focada em assistÃªncia de cÃ³digo
- **Continue.dev** - Alternativa moderna com suporte nativo para llama.cpp local (recomendado para mÃ¡quinas com VRAM limitada)
- **Interface Web** - Chat direto em http://127.0.0.1:8080/

### Fluxo RÃ¡pido (Workflow)

```
1. setup_llama_cpp_cuda124.ps1 â”€â†’ Instala binÃ¡rios + DLLs CUDA
          â†“
2. setup-models.ps1 â”€â†’ Baixa modelo (Qwen2.5 ou DeepSeek)
          â†“
3. start-llama-server.ps1 â”€â†’ Inicia servidor (porta 8080)
          â†“
4. Cline (VS Code) â”€â†’ Conecta com Base URL: http://127.0.0.1:8080/v1
          â†“
5. test-deepseek.ps1 â”€â†’ (Opcional) Valida funcionamento
```



## PrÃ©-requisitos

- **Windows 10/11** com PowerShell 5.1+
- **GPU NVIDIA** com suporte a CUDA 12.4 (RTX 3090, RTX 4090, etc.)
- **NVIDIA CUDA Toolkit 12.4** instalado
- **Git** (opcional, para clonar o repositÃ³rio)
- **aria2c** (recomendado para downloads mais rÃ¡pidos): `winget install aria2.aria2`
- MÃ­nimo **8GB de VRAM** (recomendado 16GB+ para modelos maiores)
- EspaÃ§o em disco: ~5GB para llama.cpp + modelos

---

## Scripts IncluÃ­dos

### 1. **setup_llama_cpp_cuda124.ps1**
**ConfiguraÃ§Ã£o do llama.cpp com CUDA 12.4 (Download PadrÃ£o)**

Instala o compilado binÃ¡rio do **llama.cpp** com suporte a CUDA 12.4 e as DLLs do NVIDIA CUDA Runtime necessÃ¡rias para executar modelos com aceleraÃ§Ã£o de GPU.

**O que faz:**
- Download do `llama-b8083-bin-win-cuda-12.4-x64.zip` (binÃ¡rios prÃ©-compilados)
- Download do `cudart-llama-bin-win-cuda-12.4-x64.zip` (DLLs do CUDA Runtime)
- Detecta automaticamente a pasta **Downloads** do usuÃ¡rio
- Permite escolher o disco de instalaÃ§Ã£o (C:, D:, E:, etc.)
- Extrai os arquivos para `[DISCO]:\llama-cpp-cuda124`
- Verifica e valida a instalaÃ§Ã£o

**Uso:**
```powershell
powershell -ExecutionPolicy Bypass -File .\setup_llama_cpp_cuda124.ps1
```

**SaÃ­da esperada:**
```powershell
Em qual disco deseja instalar? (Ex: C, D): C
OK - llama-b8083-bin-win-cuda-12.4-x64.zip baixado
OK - cudart-llama-bin-win-cuda-12.4-x64.zip baixado
OK - InstalaÃ§Ã£o completa em C:\llama-cpp-cuda124
```

### 2. **setup_llama_cpp_cuda12_cuda_13.ps1**
**ConfiguraÃ§Ã£o Inteligente do llama.cpp com DetecÃ§Ã£o AutomÃ¡tica de CUDA**

Script avanÃ§ado que detecta automaticamente a versÃ£o de CUDA suportada pelo driver NVIDIA e oferece escolha entre CUDA 12.4 e CUDA 13.1.

**O que faz:**
- Detecta automaticamente a versÃ£o de CUDA via `nvidia-smi`
- Oferece escolha inteligente baseada no driver instalado:
  - CUDA 13.1 (b8149) para drivers que suportam CUDA 13.x
  - CUDA 12.4 (b8083) para drivers compatÃ­veis com CUDA 12.x
- Download dos binÃ¡rios correspondentes Ã  versÃ£o escolhida
- Mesma instalaÃ§Ã£o e validaÃ§Ã£o que os scripts anteriores

**Quando usar:**
- Quando vocÃª tem drivers NVIDIA recentes e quer a versÃ£o mais otimizada
- Para evitar incompatibilidades entre versÃµes de CUDA
- Quando nÃ£o tem certeza qual versÃ£o usar

**Uso:**
```powershell
powershell -ExecutionPolicy Bypass -File .\setup_llama_cpp_cuda12_cuda_13.ps1

# SaÃ­da esperada:
# CUDA suportado pelo driver (nvidia-smi): 13.1
# SugestÃ£o com base no driver NVIDIA (nvidia-smi): opÃ§Ã£o 2
# Digite 1, 2 ou pressione ENTER para usar a sugestÃ£o (2)
```

**ComparaÃ§Ã£o de versÃµes:**
| VersÃ£o | BinÃ¡rio | Compatibilidade | Recomendado para |
|--------|---------|----------------|------------------|
| CUDA 12.4 | b8083 | Drivers mais antigos | Sistemas com drivers CUDA 12.x |
| CUDA 13.1 | b8149 | Drivers recentes | Sistemas com drivers CUDA 13.x |

### 3. **setup_llama_cpp_cuda12_cuda_13_aria2c.ps1**
**ConfiguraÃ§Ã£o com Download Acelerado via aria2c**

VersÃ£o do script inteligente com suporte a downloads paralelos e resumÃ­veis via aria2c, ideal para conexÃµes instÃ¡veis ou downloads grandes.

**O que faz:**
- Todas as funcionalidades do script inteligente (detecÃ§Ã£o automÃ¡tica)
- Download paralelo com atÃ© 16 conexÃµes simultÃ¢neas
- Resumo automÃ¡tico de downloads interrompidos
- ValidaÃ§Ã£o de integridade dos arquivos ZIP
- Fallback automÃ¡tico para Invoke-WebRequest se aria2c nÃ£o estiver disponÃ­vel

**DiferenÃ§as principais:**
- **Mais rÃ¡pido**: Downloads paralelos com mÃºltiplas conexÃµes
- **Mais robusto**: Resume downloads automaticamente
- **ValidaÃ§Ã£o**: Verifica tamanho e integridade dos ZIPs
- **Progresso**: Barra de progresso detalhada do aria2c

**Uso:**
```powershell
# Instale aria2c primeiro (opcional, mas recomendado)
winget install aria2.aria2

# Execute o script
powershell -ExecutionPolicy Bypass -File .\setup_llama_cpp_cuda12_cuda_13_aria2c.ps1
```

**SaÃ­da esperada:**
```powershell
  Iniciando download com aria2c
  URL: https://github.com/ggml-org/llama.cpp/releases/download/b8149/llama-b8149-bin-win-cuda-13.1-x64.zip
  Destino: C:\Users\usuario\Downloads\llama-b8149-bin-win-cuda-13.1-x64.zip
  A barra abaixo Ã© do prÃ³prio aria2c (progresso, velocidade, ETA)
[#0e1234 100%][#1f5678 100%][#2a9bcd 100%][#3def01 100%] 100%  5.2MB/s 0s
Download concluÃ­do com sucesso via aria2c: llama-b8149-bin-win-cuda-13.1-x64.zip
```

---

### 2. **setup_llama_cpp_cuda124_aria2c.ps1**
**ConfiguraÃ§Ã£o do llama.cpp com CUDA 12.4 (Download via aria2c)**

Mesme funcionalidade do script anterior, mas utiliza **aria2c** para downloads paralelos e resumÃ­veis (mais rÃ¡pido para conexÃµes instÃ¡veis ou downloads grandes).

**O que faz:**
- Verifica se aria2c estÃ¡ instalado
- Download paralelo dos binÃ¡rios usando aria2c
- Download paralelo das DLLs do CUDA Runtime
- Suporte a resumo automÃ¡tico se o download for interrompido
- Mesma instalaÃ§Ã£o e validaÃ§Ã£o que o script padrÃ£o

**DiferenÃ§as principais:**
- **Mais rÃ¡pido**: Downloads paralelos com atÃ© 4 conexÃµes simultÃ¢neas
- **Mais robusto**: Resume downloads interrompidos automaticamente
- **Requer aria2c**: Execute `winget install aria2.aria2` antes

**Uso:**
```powershell
winget install aria2.aria2  # Se nÃ£o tiver instalado
powershell -ExecutionPolicy Bypass -File .\setup_llama_cpp_cuda124_aria2c.ps1
```

---

### 3. **setup-models.ps1**
**Download Inteligente de Modelos GGUF**

Script interativo que permite escolher e baixar modelos de linguagem otimizados no formato GGUF da comunidade Hugging Face.

**Modelos disponÃ­veis:**

| Modelo | Tamanho | Velocidade | Capacidade | Ideal para |
|--------|---------|-----------|-----------|-----------|
| **Qwen2.5-Coder-0.5B** | ~379 MB | Muito rÃ¡pido | BÃ¡sica | Testes, prototipagem rÃ¡pida |
| **DeepSeek-Coder-6.7B** | ~4.08 GB | Moderado | AvanÃ§ada | ProduÃ§Ã£o, tarefas complexas |

**O que faz:**
- Menu interativo para seleÃ§Ã£o de modelo
- Escolha dinÃ¢mica de disco de instalaÃ§Ã£o
- Verifica se o modelo jÃ¡ existe (case-insensitive)
- Download com resumo automÃ¡tico via aria2c
- Padroniza nomes para minÃºsculas
- Organiza em estrutura: `[DISCO]:\models-ai\[modelo]\[arquivo.gguf]`

**Uso:**
```powershell
powershell -ExecutionPolicy Bypass -File .\setup-models.ps1

# Selecione:
# [1] Qwen2.5-Coder-0.5B-Instruct (~379 MB)
# [2] DeepSeek-Coder-6.7B-Instruct (~4.08 GB)
# [3] Ambos
```

**Estrutura criada:**
```
C:\models-ai\
â”œâ”€â”€ qwen2.5-coder-0.5b-instruct\
â”‚   â””â”€â”€ qwen2.5-coder-0.5b-instruct-q4_k_m.gguf
â””â”€â”€ deepseek-coder-6.7b-instruct\
    â””â”€â”€ deepseek-coder-6.7b-instruct-q4_k_m.gguf
```

---

### 4. **setup-deepseek-coder-6.7b-instruct.ps1**
**Download do DeepSeek Coder 6.7B**

Script simplificado que baixa diretamente o modelo **DeepSeek Coder 6.7B Instruct** (~4.08 GB) sem menu de seleÃ§Ã£o.

**O que faz:**
- Download direto do `deepseek-coder-6.7b-instruct-q4_k_m.gguf`
- Salva automaticamente em `C:\models-ai\deepseek-coder-6.7b-instruct\`
- Usa aria2c para download paralelo e resumÃ­vel
- Verifica integridade do arquivo apÃ³s download

**Uso:**
```powershell
powershell -ExecutionPolicy Bypass -File .\setup-deepseek-coder-6.7b-instruct.ps1
```

**Quando usar:**
- Quando vocÃª sabe que quer especificamente o DeepSeek
- Para automaÃ§Ã£o ou CI/CD
- Para evitar prompts interativos

---

### 5. **setup-qwen2.5-coder-0.5b.ps1**
**Download do Qwen2.5 Coder 0.5B**

Script simplificado que baixa diretamente o modelo **Qwen2.5 Coder 0.5B Instruct** (~379 MB) sem menu de seleÃ§Ã£o.

**O que faz:**
- Download direto do `qwen2.5-coder-0.5b-instruct-q4_k_m.gguf`
- Salva automaticamente em `C:\models-ai\qwen2.5-coder-0.5b-instruct\`
- Usa aria2c para download paralelo e resumÃ­vel
- Muito mais rÃ¡pido que DeepSeek (ideal para testes)

**Uso:**
```powershell
powershell -ExecutionPolicy Bypass -File .\setup-qwen2.5-coder-0.5b.ps1
```

**Quando usar:**
- Testes rÃ¡pidos e prototipagem
- MÃ¡quinas com VRAM limitada (< 8GB)
- AutomaÃ§Ã£o sem interaÃ§Ã£o do usuÃ¡rio

---

### 6. **start-llama-server.ps1**
**Inicia o Servidor llama.cpp com API OpenAI compatÃ­vel**

Script que inicializa um servidor HTTP rodando **llama-server** (parte do llama.cpp) expondo uma API idÃªntica Ã  OpenAI.

**Onde fica o Context no script:**
```powershell
$MODELS = @(
    @{
        Name          = 'Qwen2.5 Coder 0.5B (rapido, leve)'
        Path          = 'C:\models-ai\qwen2.5-coder-0.5b-instruct\qwen2.5-coder-0.5b-instruct-q4_k_m.gguf'
        ID            = 'qwen2.5-coder-0.5b-instruct-q4_k_m.gguf'
        Context       = 16384  # ALTERE AQUI PARA OTIMIZAR (8192, 4096, etc)
        Template      = 'qwen'
        DefaultTemp   = 0.4
        DefaultRepeat = 1.3
    },
    @{
        Name          = 'DeepSeek Coder 6.7B (mais capaz, mais lento)'
        Path          = 'C:\models-ai\deepseek-coder-6.7b-instruct\deepseek-coder-6.7b-instruct-q4_k_m.gguf'
        ID            = 'deepseek-coder-6.7b-instruct-q4_k_m.gguf'
        Context       = 16384  # ALTERE AQUI PARA OTIMIZAR (8192, 4096, etc)
        Template      = 'deepseek-coder-chat-template.jinja'
        DefaultTemp   = 0.1
        DefaultRepeat = 1.1
    }
)
```

**O que faz:**
- Menu interativo para seleÃ§Ã£o do modelo (Qwen2.5 ou DeepSeek)
- Valida se o modelo existe antes de iniciar
- Inicia `llama-server` na porta `8080`
- Suporta templates especÃ­ficos por modelo:
  - **Qwen**: Template nativo do Qwen
  - **DeepSeek**: Template customizado (`deepseek-coder-chat-template.jinja`)
- ParametrizaÃ§Ãµes dinÃ¢micas:
  - Temperatura (controla criatividade)
  - Context window (tamanho do histÃ³rico)
  - Repeat penalty (evita repetiÃ§Ã£o)
- Log do servidor em `%TEMP%\llama-server.log`

**ConfiguraÃ§Ãµes por modelo:**

| Parametro | Qwen2.5 | DeepSeek |
|-----------|---------|----------|
| **Context Window** | 16384 tokens | 16384 tokens |
| Temperature PadrÃ£o | 0.4 | 0.1 |
| Repeat Penalty | 1.3 | 1.1 |
| Template | qwen | deepseek-coder-chat-template.jinja |

#### ğŸ“Œ Entendendo o `Context Window` (16384 tokens)

**O que Ã©:**
- Context Window Ã© o tamanho mÃ¡ximo da "memÃ³ria" do modelo
- 16384 tokens â‰ˆ ~12.000 caracteres ou ~2.000 linhas de cÃ³digo
- Inclui: histÃ³rico da conversa + prompt + sua mensagem atual

**Por que Ã© importante:**
- Maior context = Modelo lembra de mais contexto anterior
- Ideal para conversas longas ou anÃ¡lise de projetos grandes
- Menor context = Respostas mais rÃ¡pidas, menos VRAM usado
- Se ultrapassar o limite, as mensagens antigas sÃ£o descartadas

**RelaÃ§Ã£o com Cline (VS Code):**
```
Context Window (16384) = Max Tokens TOTAL (histÃ³rico + resposta)
Max Tokens (Cline) = Tokens para APENAS a resposta
```

**RecomendaÃ§Ãµes:**
- Deixe **Max Tokens no Cline em ~2048 a 4096**
- Assim sobra espaÃ§o para histÃ³rico: `16384 - 4096 = 12288 tokens para contexto anterior`

**Uso:**
```powershell
powershell -ExecutionPolicy Bypass -File .\start-llama-server.ps1

# Selecione:
# [1] Qwen2.5 Coder 0.5B (rÃ¡pido, leve)
# [2] DeepSeek Coder 6.7B (mais capaz, mais lento)

# Output esperado:
# OK - Servidor iniciado em http://127.0.0.1:8080
# OK - Modelo: deepseek-coder-6.7b-instruct-q4_k_m.gguf
```

**Endpoints disponÃ­veis:**
- `POST /completion` - Endpoint raw com template manual
- `POST /v1/chat/completions` - Endpoint OpenAI (usado pelo Cline)
- `GET /health` - Verificar status do servidor

**Interface Web de Chat:**

O servidor llama.cpp fornece uma **interface de chat web** interativa disponÃ­vel em:

```
http://127.0.0.1:8080/
```

VocÃª pode acessar pelo navegador enquanto o servidor estÃ¡ rodando. Esta interface permite:
- Chat interativo direto com o modelo
- Ajuste de parÃ¢metros (temperatura, max tokens, etc.)
- VisualizaÃ§Ã£o de respostas em tempo real
- Teste completo do modelo sem precisar de Cline ou API

**Acesso em Rede Local:**

O servidor agora escuta em todas as interfaces (`0.0.0.0`), permitindo acesso de outros dispositivos na mesma rede:

```
http://192.168.50.1:8080/  # Substitua pelo IP da sua mÃ¡quina
```

**Como acessar:**
1. Execute: `.\start-llama-server.ps1`
2. Abra no navegador: `http://127.0.0.1:8080/` (local) ou `http://[SEU-IP]:8080/` (rede)
3. Comece a conversar com o modelo

**Para descobrir seu IP local:**
```powershell
ipconfig | findstr "IPv4"
```

---

### 7. **test-deepseek.ps1**
**Teste de Funcionalidade do Modelo DeepSeek**

Script de teste que valida se o servidor estÃ¡ funcionando corretamente, testando os dois endpoints principais com prompts reais.

**O que faz:**
- Testa **2 tarefas de programaÃ§Ã£o** diferentes
- Usa **2 endpoints distintos**:
  1. `/completion` (endpoint raw com template manual)
  2. `/v1/chat/completions` (endpoint OpenAI - usado pelo Cline)
- Salva resultados em `test-deepseek-result.txt`
- Exibe progresso no console e no arquivo

**Prompts de teste:**
1. "Write a Python function to check if a number is prime."
2. "Write a Python function that receives a list of integers and returns only the even numbers."

**ConfiguraÃ§Ãµes:**
- Temperatura: 0.1 (determinÃ­stico)
- Max tokens: 300 (limite de resposta)
- Modelo: deepseek-coder-6.7b-instruct

**Uso (com servidor rodando):**
```powershell
# Em outro terminal PowerShell:
powershell -ExecutionPolicy Bypass -File .\test-deepseek.ps1

# SaÃ­da em test-deepseek-result.txt:
# OK - /completion - Resposta 1: ...
# OK - /v1/chat/completions - Resposta 1: ...
# OK - /completion - Resposta 2: ...
# OK - /v1/chat/completions - Resposta 2: ...
```

---

### 8. **deepseek-coder-chat-template.jinja**
**Template Jinja2 para Format do DeepSeek Coder**

Template de formataÃ§Ã£o que adapta as mensagens do chat para o formato esperado pelo modelo DeepSeek Coder.

**O que faz:**
- Define o sistema de prompt padrÃ£o do DeepSeek
- Formata mensagens de usuÃ¡rio como `### Instruction:`
- Formata respostas como `### Response:`
- Garante alternÃ¢ncia correta de papÃ©is (user/assistant)
- Suporta system message customizada

**ConteÃºdo:**
```jinja
{{- bos_token }}
{%- if messages[0]['role'] == 'system' %}
    {%- set system_message = messages[0]['content'] %}
{%- else %}
    {%- set system_message = 'You are an AI programming assistant...' %}
{%- endif %}

{%- for message in loop_messages %}
    {%- if message['role'] == 'user' %}
        {{- '### Instruction:\n' + message['content'] + '\n' }}
    {%- elif message['role'] == 'assistant' %}
        {{- '### Response:\n' + message['content'] + '\n<|EOT|>\n' }}
    {%- endif %}
{%- endfor %}
```

**Usado por:**
- `start-llama-server.ps1` quando DeepSeek Ã© selecionado
- Garante formataÃ§Ã£o correta das conversas no endpoint `/v1/chat/completions`

### 9. **start-llama-server-rtx4050.ps1**
**Servidor Otimizado para RTX 4050 (6GB VRAM)**

Script especializado que configura o servidor llama.cpp com otimizaÃ§Ãµes especÃ­ficas para GPUs RTX 4050 com 6GB de VRAM, incluindo Flash Attention e gerenciamento avanÃ§ado de memÃ³ria.

**O que faz:**
- **Flash Attention**: Reduz consumo de VRAM em ~40% no cache KV e aumenta velocidade
- **KV Cache Quantizado**: Usa q8_0 para reduzir VRAM em ~50% vs FP16 com qualidade prÃ³xima
- **GPU Layer Offload Total**: Move todas as camadas do modelo para a GPU (n-gpu-layers 99)
- **Context Window Ajustado**: DeepSeek usa 8192 tokens para caber nos 6GB da RTX 4050
- **Batching Otimizado**: Batch size maior (2048) para melhor throughput na GPU
- **Paralelismo Inteligente**: 1 slot para Cline (single-user) com batching continuo

**ConfiguraÃ§Ãµes por modelo (RTX 4050 6GB):**

| Modelo | Context | VRAM Est. | GPU Layers | KV Cache | OtimizaÃ§Ãµes |
|--------|---------|-----------|------------|----------|-------------|
| **Qwen2.5 0.5B** | 16384 | ~0.8 GB | 99 | q8_0 | Offload total, Flash Attention |
| **DeepSeek 6.7B** | 8192 | ~5.8 GB | 99 | q8_0 | Context reduzido, Flash Attention |

**ParÃ¢metros crÃ­ticos RTX 4050:**
```powershell
# Flash Attention (requer Ada Lovelace cc 8.9 - RTX 4050 suporta)
--flash-attn

# KV cache quantizado (economiza ~50% VRAM)
--cache-type-k q8_0
--cache-type-v q8_0

# Offload total para GPU
--n-gpu-layers 99

# Batch maior para melhor uso dos Tensor Cores
--batch-size 2048
--ubatch-size 512

# Paralelismo single-user (Cline)
--parallel 1
--cont-batching
```

**Quando usar:**
- GPU RTX 4050 (Ada Lovelace, 6GB VRAM)
- Quer mÃ¡ximo desempenho com Flash Attention
- Precisa gerenciar VRAM de forma inteligente
- Usa principalmente Cline (single-user)

**Uso:**
```powershell
powershell -ExecutionPolicy Bypass -File .\start-llama-server-rtx4050.ps1

# Selecione:
# [1] Qwen2.5 Coder 0.5B (rÃ¡pido, leve)
# [2] DeepSeek Coder 6.7B (mais capaz, mais lento)

# Output esperado:
# OtimizaÃ§Ãµes RTX 4050 ativas:
#   --n-gpu-layers 99 -> offload TOTAL do modelo para a GPU
#   --flash-attn -> Flash Attention ativo (Ada Lovelace suportado)
#   --cache-type-k/v q8_0 -> KV cache quantizado em 8 bits (-50% VRAM vs FP16)
```

**Monitoramento RTX 4050:**
```powershell
# Uso da GPU em tempo real
nvidia-smi -l 1

# Detalhes de VRAM e utilizaÃ§Ã£o
nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.free --format=csv -l 2

# MÃ©tricas do llama.cpp
curl http://127.0.0.1:8080/metrics

# Logs ao vivo
Get-Content $env:TEMP\llama-server-rtx.log -Tail 50 -Wait
```

**ComparaÃ§Ã£o com start-llama-server.ps1:**
| Feature | start-llama-server.ps1 | start-llama-server-rtx4050.ps1 |
|---------|------------------------|--------------------------------|
| **Flash Attention** | NÃ£o | Sim (Ada Lovelace) |
| **KV Cache** | FP16 | q8_0 (50% menos VRAM) |
| **GPU Layers** | ConfigurÃ¡vel | 99 (total) |
| **Context DeepSeek** | 16384 | 8192 (ajustado para 6GB) |
| **Batch Size** | 512 | 2048 (otimizado GPU) |
| **VRAM Est. DeepSeek** | ~12GB | ~5.8GB |

### 10. **.continue/continue.config.yaml**
**ConfiguraÃ§Ã£o PrÃ©-Configurada para Continue.dev**

Arquivo de configuraÃ§Ã£o YAML que define automaticamente o servidor llama.cpp local para uso com a extensÃ£o Continue.dev no VS Code.

**O que faz:**
- Define o modelo DeepSeek Coder 6.7B como padrÃ£o
- Configura a URL do servidor local (`http://127.0.0.1:8080/v1`)
- Define a chave de API fictÃ­cia (nÃ£o verificada)
- Pronta para uso sem ediÃ§Ãµes adicionais

**ConteÃºdo:**
```yaml
name: DeepSeek Coder 6.7B
version: 1.0.0

models:
  - name: DeepSeek Coder Local
    provider: openai
    model: deepseek-coder-6.7b-instruct-q4_k_m.gguf
    apiBase: http://127.0.0.1:8080/v1
    apiKey: sk-no-key-required
```

**Como usar:**
1. Instale a extensÃ£o [Continue.dev](https://marketplace.visualstudio.com/items?itemName=Continue.continue)
2. Continue detecta automaticamente este arquivo ao abrir o workspace
3. Pronto para usar!

---

## Fluxo de Trabalho Recomendado

### Primeira ConfiguraÃ§Ã£o (One-time setup):

1. **Instalar aria2c** (recomendado)
   ```powershell
   winget install aria2.aria2
   ```

2. **Instalar llama.cpp com CUDA** (escolha inteligente):
   ```powershell
   # OpÃ§Ã£o recomendada: DetecÃ§Ã£o automÃ¡tica de versÃ£o
   .\setup_llama_cpp_cuda12_cuda_13.ps1
   
   # Ou com downloads acelerados:
   .\setup_llama_cpp_cuda12_cuda_13_aria2c.ps1
   
   # OpÃ§Ãµes tradicionais (se preferir):
   .\setup_llama_cpp_cuda124.ps1          # CUDA 12.4 fixo
   .\setup_llama_cpp_cuda124_aria2c.ps1   # CUDA 12.4 com aria2c
   ```

   **Escolha da versÃ£o de CUDA:**
   - **CUDA 13.1**: Drivers NVIDIA recentes (RTX 40xx, drivers 550+)
   - **CUDA 12.4**: Drivers mais antigos ou compatibilidade garantida
   - O script inteligente detecta automaticamente a versÃ£o suportada pelo seu driver

3. **Fazer download dos modelos**:
   ```powershell
   .\setup-models.ps1  # Menu interativo
   # Ou escolha especÃ­fico:
   .\setup-qwen2.5-coder-0.5b.ps1      # Para testes rÃ¡pidos
   .\setup-deepseek-coder-6.7b-instruct.ps1  # Para produÃ§Ã£o
   ```

### Uso DiÃ¡rio:

4. **Iniciar o servidor** (em um terminal):
   ```powershell
   # Para uso geral:
   .\start-llama-server.ps1
   
   # Para RTX 4050 (6GB VRAM) - mÃ¡ximo desempenho:
   .\start-llama-server-rtx4050.ps1
   
   # Selecione o modelo desejado
   # Servidor estarÃ¡ em http://127.0.0.1:8080
   ```

   **Escolha do script de servidor:**
   - **start-llama-server.ps1**: Uso geral, compatÃ­vel com todas as GPUs
   - **start-llama-server-rtx4050.ps1**: Otimizado para RTX 4050, Flash Attention, gerenciamento avanÃ§ado de VRAM

5. **Usar no Cline** (VS Code):
   - Abra as configuraÃ§Ãµes do Cline
   - Clique em **"Connect to a model"** â†’ **"OpenAI Compatible"**
   - Preencha os campos conforme tabela abaixo

**Tabela de ConfiguraÃ§Ã£o do Cline:**

| Campo | Valor | DescriÃ§Ã£o | NecessÃ¡rio? |
|-------|-------|-----------|-------------|
| **Provider** | `OpenAI Compatible` | API compatÃ­vel com OpenAI | Sim |
| **Base URL** | `http://127.0.0.1:8080/v1` | URL do servidor llama.cpp + `/v1` | Sim |
| **API Key** | `sk-no-key-required` | Chave fictÃ­cia (nÃ£o verificada) | Sim |
| **Model** | `deepseek-coder-6.7b-instruct-q4_k_m.gguf` | Mesmo nome do modelo (ou qwen2.5...) | Sim |
| **Max Tokens** | `4096` | MÃ¡ximo de tokens por resposta | IMPORTANTE |

**CRÃTICO - Max Tokens vs Context Window:**

```
Context Window do modelo (servidor):  16384 tokens TOTAL
â”œâ”€â”€ HistÃ³rico da conversa:            ~8288 tokens
â”œâ”€â”€ Sua pergunta atual:               ~2000 tokens
â””â”€â”€ Max Tokens no Cline (resposta):   4096 tokens (MÃXIMO)
                                     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                    TOTAL: 14384 OK
```

**RecomendaÃ§Ãµes de Max Tokens:**
- DeepSeek 6.7B: `4096` tokens (balanceado entre qualidade e velocidade)
- Qwen2.5 0.5B: `2048` tokens (modelo menor, menos tokens necessÃ¡rios)
- MÃ¡ximo seguro: Nunca exceda `12000` (deixa pouco espaÃ§o para histÃ³rico)

**Teste a configuraÃ§Ã£o:**
```powershell
# Em PowerShell, verifique se servidor responde:
Invoke-WebRequest http://127.0.0.1:8080/v1/models -UseBasicParsing | ConvertFrom-Json
# Deve mostrar os modelos disponÃ­veis
```

#### Alternativa: Continue.dev (Moderna e Local-First)

**Continue.dev** Ã© uma alternativa moderna ao Cline com suporte nativo para llama.cpp local.

**InstalaÃ§Ã£o:**
- Abra o VS Code Marketplace: Search "Continue"
- Instale a extensÃ£o [Continue - continue.dev](https://marketplace.visualstudio.com/items?itemName=Continue.continue)
- Reload VS Code

**ConfiguraÃ§Ã£o:**
- Este repositÃ³rio jÃ¡ inclui arquivo de configuraÃ§Ã£o pronto em `.continue/continue.config.yaml`
- Continue detecta automaticamente este arquivo ao abrir o workspace
- Nenhuma configuraÃ§Ã£o adicional necessÃ¡ria!

**Tabela de ConfiguraÃ§Ã£o do Continue.dev:**

| Campo | Valor |
|-------|-------|
| Provider | openai |
| Model | deepseek-coder-6.7b-instruct-q4_k_m.gguf |
| API Base | http://127.0.0.1:8080/v1 |
| API Key | sk-no-key-required |

**Arquivo de ConfiguraÃ§Ã£o:**
O arquivo `.continue/continue.config.yaml` jÃ¡ estÃ¡ prÃ©-configurado:
```yaml
name: DeepSeek Coder 6.7B
version: 1.0.0

models:
  - name: DeepSeek Coder Local
    provider: openai
    model: deepseek-coder-6.7b-instruct-q4_k_m.gguf
    apiBase: http://127.0.0.1:8080/v1
    apiKey: sk-no-key-required
```

---

**Se Max Tokens for muito alto:**
- Resposta fica muito longa (e lenta)
- Pouco espaÃ§o para histÃ³rico anterior
- Alto consumo de VRAM

**Se Max Tokens for muito baixo:**
- Respostas incompletas ou truncadas
- Mais rÃ¡pido
- Menos VRAM usado

> **Dica Pro:** Comece com `Max Tokens = 4096` no Cline. Se as respostas forem muito curtas, aumente para `6144`. Se ficar lento, reduza para `2048`.

6. **Testar (opcional)**:
   ```powershell
   # Em outro terminal
   .\test-deepseek.ps1
   ```

---

## ParÃ¢metros e PersonalizaÃ§Ã£o

> **Aviso Importante:** Os 3 parÃ¢metros mais crÃ­ticos sÃ£o `Context`, `DefaultTemp` e `DefaultRepeat`. NÃ£o modifique sem entender o impacto!

### Os 3 ParÃ¢metros CrÃ­ticos

| ParÃ¢metro | O que faz | Impacto em VRAM | Impacto em Velocidade | PadrÃ£o |
|-----------|-----------|----------------|-----------------------|--------|
| **Context** | Tamanho da memÃ³ria do modelo | ALTO | ALTO | 16384 |
| **DefaultTemp** | Criatividade (0.0 = determinÃ­stico, 2.0 = criativo) | Nenhum | Nenhum | 0.1-0.4 |
| **DefaultRepeat** | Penalidade para evitar repetiÃ§Ã£o (1.0 = sem penalidade) | Nenhum | Nenhum | 1.1-1.3 |



O `Context` Ã© o fator mais importante para performance. Edite `start-llama-server.ps1`:

```powershell
# MUITO GRANDE (lento, alto uso de VRAM)
Context = 32768

# RECOMENDADO (balanceado)
Context = 16384  # PadrÃ£o

# PARA MÃQUINAS COM POUCA VRAM
Context = 8192   # 50% mais rÃ¡pido, menos memÃ³ria

# PARA TESTES RÃPIDOS
Context = 4096   # Muito rÃ¡pido, pouco histÃ³rico
```

**Impacto do Context Window:**

| Context | VRAM (DeepSeek) | Velocidade | HistÃ³rico | Melhor para |
|---------|-----------------|-----------|-----------|-----------|
| 4096 | ~6GB | Muito rÃ¡pido | Curto | Testes, protÃ³tipos |
| 8192 | ~8GB | RÃ¡pido | MÃ©dio | Balanceado |
| 16384 | ~12GB | Moderado | Longo | ProduÃ§Ã£o, conversas longas |
| 32768 | ~20GB | Lento | Muito longo | AnÃ¡lise de projetos grandes |

**CÃ¡lculo de VRAM usado:**
```
VRAM (GB) â‰ˆ (Tamanho do modelo em GB) + (Context / 1000)
```

**Exemplo DeepSeek 6.7B:**
- Modelo base: ~6.7GB
- Context 16384: ~6.7 + 16.4 â‰ˆ **23.1 GB VRAM**
- Context 8192: ~6.7 + 8.2 â‰ˆ **14.9 GB VRAM**
- Context 4096: ~6.7 + 4.1 â‰ˆ **10.8 GB VRAM**

### Ajustar Temperatura (Criatividade)

```powershell
# Mais determinÃ­stico (0.0-0.5)
DefaultTemp = 0.1  # Respostas previsÃ­veis

# Equilibrado (0.5-1.0)
DefaultTemp = 0.7  # Bom para geral

# Mais criativo (1.0-2.0)
DefaultTemp = 1.3  # Respostas diversas
```

### Ajustar Context Window

```powershell
Context = 16384  # Janela de contexto em tokens
# Quanto maior, mais histÃ³rico de conversa, mas mais lenta a inferÃªncia
```

### Usar Template Customizado

Copie `deepseek-coder-chat-template.jinja` para outra localizaÃ§Ã£o e modifique:

```powershell
Template = 'C:\custom\meu-template.jinja'
```

### Outros Modelos GGUF

Para adicionar novo modelo no `start-llama-server.ps1`:

```powershell
$MODELS = @(
    # ... modelos existentes ...
    @{
        Name          = 'Novo Modelo Local'
        Path          = 'C:\models-ai\novo-modelo\modelo.gguf'
        ID            = 'novo-modelo-q4_k_m.gguf'
        Context       = 8192
        Template      = 'default'
        DefaultTemp   = 0.7
        DefaultRepeat = 1.1
    }
)
```

---

## SoluÃ§Ã£o de Problemas

### Erro: "aria2c nÃ£o encontrado"
```powershell
winget install aria2.aria2
# Reinicie o PowerShell apÃ³s instalar
```

### Erro: "Pasta Downloads nÃ£o encontrada"
O script tenta detectar automaticamente. Se falhar, edite e defina:
```powershell
$DOWNLOADS_DIR = "C:\Users\SeuUsuÃ¡rio\Downloads"
```

### GPU nÃ£o estÃ¡ sendo usada
Verifique instalaÃ§Ã£o do CUDA 12.4:
```powershell
nvidia-smi  # Deve mostrar sua GPU e driver
nvcc --version  # Deve mostrar CUDA 12.4.x
```

### Download muito lento
- Use `setup_llama_cpp_cuda124_aria2c.ps1` para downloads paralelos
- Verifique sua conexÃ£o de internet
- Considere fazer download manual e mover para a pasta apropriada

### Servidor nÃ£o inicia
- Verifique se porta 8080 estÃ¡ livre: `netstat -an | findstr 8080`
- Verifique se o modelo existe: `Test-Path C:\models-ai\...`
- Consulte o log: `cat $env:TEMP\llama-server.log`

### Resposta muito lenta ou trava

**Primeira aÃ§Ã£o: Reduzir o Context Window!**

```powershell
# Em start-llama-server.ps1, reduza:
Context = 16384  # Altere para:
Context = 8192   # Ou atÃ© 4096 se muito lento
```

- Reduz o `Context` de 16384 para 8192 ou 4096
- Use Qwen2.5 em vez de DeepSeek (muito mais rÃ¡pido)
- Feche outros aplicativos que usam VRAM
- Monitore com `nvidia-smi -l 1` em outro terminal

**Checklist de otimizaÃ§Ã£o:**
- Context reduzido para 8192 ou 4096?
- Temperatura baixa (0.1-0.4 Ã© ideal)?
- Outras aplicaÃ§Ãµes fechadas?
- VRAM disponÃ­vel >= 8GB?
- Usando Qwen2.5 para testes?

### Cline nÃ£o se conecta ao servidor
```powershell
# Teste manualmente:
Invoke-WebRequest http://127.0.0.1:8080/health -UseBasicParsing
# Deve retornar {"status":"ok"}
```

---

## ComparaÃ§Ã£o de Modelos

| Aspecto | Qwen2.5-Coder-0.5B | DeepSeek-Coder-6.7B |
|---------|-------------------|-------------------|
| **Tamanho** | 379 MB | 4.08 GB |
| **VRAM requerida** | ~2GB | ~8GB |
| **Velocidade** | Muito rÃ¡pido | Moderado |
| **Qualidade** | BÃ¡sica | AvanÃ§ada |
| **Tempo/token** | ~0.1s | ~0.5s |
| **Ideal para** | Testes, prototipagem | ProduÃ§Ã£o, anÃ¡lise |

---

## ReferÃªncias e Links

- [llama.cpp GitHub](https://github.com/ggml-org/llama.cpp)
- [Modelos GGUF no Hugging Face](https://huggingface.co/models?search=gguf)
- [aria2c Documentation](https://aria2.github.io/)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Cline VS Code Extension](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev)

---

## Context Window vs Cline Max Tokens (Guia RÃ¡pido)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CONTEXT WINDOW DO SERVIDOR (16384)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  HistÃ³Â­rico anterior:          ~8288 tokens        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  â€¢ Mensagens da conversa antiga                    â”‚
â”‚  â€¢ Contexto de arquivos anteriores                 â”‚
â”‚  â€¢ Respostas prÃ©vias do modelo                    â”‚
â”‚                                                     â”‚
â”‚  Sua mensagem atual:           ~2000 tokens       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  â€¢ Pergunta que vocÃª estÃ¡ fazendo                â”‚
â”‚  â€¢ CÃ³digo que enviou                             â”‚
â”‚                                                     â”‚
â”‚  Resposta do modelo (Cline):   4096 tokens (MAX)  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  â€¢ O que o Cline exibe como resposta             â”‚
â”‚  â€¢ ConfigurÃ¡vel em Max Tokens                    â”‚
â”‚                                                     â”‚
â”‚  TOTAL: 8288 + 2000 + 4096 = 14384 tokens OK      â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**RecomendaÃ§Ã£o Final:**
- Context no servidor: `16384` (nÃ£o mude sem necessidade)
- Max Tokens no Cline: `4096` (comeÃ§a com este valor)
- Se ficar lento: Reduz Max Tokens para `2048` ou Context para `8192`

---

- Todos os scripts requerem PowerShell com `ExecutionPolicy Bypass`
- CUDA 12.4 Ã© obrigatÃ³rio; versÃµes anteriores nÃ£o funcionarÃ£o
- A primeira execuÃ§Ã£o de cada script serÃ¡ mais lenta (downloads)
- Modelos sÃ£o cacheados localmente apÃ³s primeiro download
- O servidor llama.cpp roda indefinidamente atÃ© ser interrompido (Ctrl+C)

### Lembrete de ParÃ¢metros (Context vs Max Tokens)

**No servidor (start-llama-server.ps1):**
- `Context = 16384` = Tamanho total da memÃ³ria do modelo
- Impacto direto em VRAM e velocidade
- Reduza para `8192` ou `4096` se ficar lento

**No Cline (VS Code):**
- Max Tokens = 4096 = Tamanho mÃ¡ximo da RESPOSTA
- Deve ser menor que o Context do servidor
- Recomendado: 4096 para DeepSeek, 2048 para Qwen2.5

**ValidaÃ§Ã£o rÃ¡pida:**
```powershell
# Teste se servidor estÃ¡ respondendo:
Invoke-WebRequest http://127.0.0.1:8080/v1/models -UseBasicParsing
# Deve retornar lista de modelos disponÃ­veis
```

---

**VersÃ£o:** 1.0  
**Ãšltima atualizaÃ§Ã£o:** Fevereiro 2026  
**Autor:** Fernando Padilha Avena