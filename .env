# Configurações do servidor llama.cpp
# URL do servidor llama.cpp (pode ser localhost ou IP como 192.168.50.1)
# Nota: Incluir /v1 no final para compatibilidade com API OpenAI
LLAMA_SERVER_URL=http://192.168.50.1:8080


# Configurações padrão (podem ser sobrescritas por modelo)
DEFAULT_MAX_TOKENS=2048
DEFAULT_TEMPERATURE=0.2